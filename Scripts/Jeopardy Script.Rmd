---
title: "Dataquest Jeopardy! Project"
author: "Douglas Carr"
date: "5/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Project Purpose

The purpose of this project is to use data regarding Jeopary! show questions to determine the optimal categories to study to provide the best chance of success if you appear on the show.  We will do this by examining a data set of 20,000 questions from Jeopardy! episodes and attempting to discern any pattern regarding question categories.

## Data Import

```{r}
jeopardy <- read_csv("https://query.data.world/s/gb5qjrgib6gg3dz24udelbh7jqmpjt")
```

Now that the data is loaded, let's examine the first few rows of the dataset plus the column names list to plot out our next steps.

```{r}
head(jeopardy, 5)
colnames(jeopardy)
```

## Data Prep

The column names are decidedly un-tidy, so let's use the `janitor` package to clean that up!

```{r include=FALSE}
library(janitor)
```

```{r}
jeopardy <- jeopardy %>%
  janitor::clean_names()
```

Now that we've got our column names nice and tidy, we need to fix the value column to be numeric rather than character. The presence of the dollar sign ($) is throwing us off, so let's get rid of it in order to convert the column. We also want to get rid of the questions with "None" as a value, as they won't work in our later analysis.

```{r}
jeopardy <- jeopardy %>%
  filter(value != "None") %>%
  mutate(
    value = str_replace_all(.$value, "[$,]", ""),
    value = as.numeric(value)
  )
```

Now let's quickly check to see if that did the trick.

```{r}
sort(unique(jeopardy$value))
typeof(jeopardy$value)
```

Looking good!

## Text Normalization

Text data can be messy to work with in the way we want to, so next we need to perform some normalization steps to make sure everything is set up the way we need it for our analysis. Specifically, we want to convert everything to lower case and then remove all punctuation marks, and we need to do this on `question`, `answer`, and `category`.

```{r}
jeopardy <- jeopardy %>%
  mutate(
    question = tolower(question),
    question = str_remove_all(question, "[^A-Za-z0-9 ]"),
    answer = tolower(answer),
    answer = str_remove_all(answer, "[^A-Za-z0-9 ]"),
    category = tolower(category),
    category = str_remove_all(category, "[^A-Za-z0-9 ]")
  )
```

We also want to separate the `air_date` column out into its constituent parts and create `year`, `month`, and `day` columns.

```{r}
jeopardy <- jeopardy %>%
  separate(air_date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day)
  )
```

Once more let's take a quick look to see if our data is looking like we expect it to.

```{r}
head(jeopardy)
```

Great! Moving along then.

## Focus on Categories

Now it's time to find out what categories we want to focus our attention on to. Anecdotally, it seems like science, history, and Shakespeare are more common on the show, so let's focus on them. First we need to find out how many unique categories we have in our data set.

```{r}
length(unique(jeopardy$category))
```

Looks like there are 3,369 unique categories. That means that, if there was no category more frequent than the others, we'd expect the odds of encountering any one category to be $\frac{1}{3369}$ (and on the flip side, the odds of *not* encountering a specific category to be $\frac{3368}{3369}$).

we're going to run an hypothesis test on the categories to determine if science, history, and Shakespeare are in fact over-representing in the data set, so we need to establish our baseline hypotheses. Our null hypothesis $H_{0}$ will be that there is no increased prevalence of any one category in the data set, or in other words all categories have the same odds of $\frac{1}{3369}$, meaning our alternative hypothesis is that the categories are not evenly distributied and thus the odds of encountering any one of our highlighted categories is in fact greater than $\frac{1}{3369}$.

This looks like it's going to be a **chi-squared test**, so let's set up our baseline proportions as variables to make this easier to run.

```{r}
n_questions <- nrow(jeopardy)
p_category_expected <- 1/3369
p_not_category_expected <- 3368/3369
p_expected <- c(p_category_expected, p_not_category_expected)
```

```{r}
categories <- pull(jeopardy, category)
n_science <- 0

# Count how many times the word "science" appears in the categories
for (c in categories) {
  if ("science" %in% c) {
    n_science <- n_science + 1
  }
}

# Run chi-sq test
science_obs <- c(n_science, n_questions - n_science)
chisq.test(science_obs, p = p_expected)
```

That is an extremely small p-value, well under the threshold level of 0.05. Therefore we are going to reject the null hypothesis and say that the evidence does not support the conclusion that science has an equal liklihood to appear as all other categories. Let's check history and Shakespeare next.

```{r}
n_history <- 0

# Count how many times the word "science" appears in the categories
for (c in categories) {
  if ("history" %in% c) {
    n_history <- n_history + 1
  }
}

# Run chi-sq test
history_obs <- c(n_history, n_questions - n_history)
chisq.test(history_obs, p = p_expected)
```

```{r}
n_shakespeare <- 0

# Count how many times the word "science" appears in the categories
for (c in categories) {
  if ("shakespeare" %in% c) {
    n_shakespeare <- n_shakespeare + 1
  }
}

# Run chi-sq test
shakespeare_obs <- c(n_shakespeare, n_questions - n_shakespeare)
chisq.test(shakespeare_obs, p = p_expected)
```

History and Shakespeare both also have p-values well under 0.05, so just like with science we are going to reject the null hypothesis for these categories as well.

## Repeated Questions

Given the sheer volume of questions on Jeopardy, its likely there are going to be some repeats, so our next step is going to be trying to identify those. We can't just search for duplicates in the questions list because any subtle wording change wouldn't be caught even if what the question was actually asking was the same. Instead, we'll filter out any short and connecting words and count up what the most commonly used words are in the questions.

```{r}
questions <- pull(jeopardy, question)
terms_used <- character(0)

for (q in questions) {
  # Split sentence into distinct words
  split_sentence <- str_split(q, " ")[[1]]
  
  # Remove words shorter than 6 characters or that are already in the vector
  for (term in split_sentence) {
    if(!term %in% terms_used & nchar(term) >=6) {
      terms_used <- c(terms_used, term)
    }
  }
}
```